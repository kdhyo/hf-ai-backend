# ============================================
# HF Week1 API — PyCharm HTTP Client definitions
# 사용법:
# 1) 이 파일을 열고 각 블록 위의 ▶ 버튼으로 실행
# 2) 상단 변수 @baseUrl 이나 환경파일(http-client.env.json)의 baseUrl을 상황에 맞게 변경
# 3) 응답은 하단 창에서 JSON으로 확인
# ============================================

@baseUrl = http://127.0.0.1:8000

### health — 서버 상태/디바이스 정보 확인
# 기대: 200 OK / {"ok": true, "torch": "...", "mps": ...}
GET {{baseUrl}}/health
Accept: application/json

### generate — 텍스트 생성 (distilgpt2)
# 파라미터:
#   prompt: 시작 문장
#   max_new_tokens: 생성 길이
#   temperature: 창의성(0.0~2.0)
POST {{baseUrl}}/generate
Content-Type: application/json

{
  "prompt": "Once upon a time",
  "max_new_tokens": 40,
  "temperature": 0.8
}

### sentiment — 감정분석
# 영어 문장에 대해 {label, score} 반환
POST {{baseUrl}}/sentiment
Content-Type: application/json

{
  "text": "I love Hugging Face!"
}

### translate — 번역(영→한)
# Marian/NLLB 기반. 서버 쪽은 NLLB(eng_Latn→kor_Hang) 설정 권장
POST {{baseUrl}}/translate/en2ko
Content-Type: application/json

{
  "texts": ["How are you?", "This API works great."],
  "num_beams": 4,
  "max_length": 128,
  "min_length": 1
}

### summarize (EN) — 영어 요약 (BART)
# max_length/min_length: '출력 요약' 길이 제어 (토큰 기준)
POST {{baseUrl}}/summarize/en
Content-Type: application/json

{
  "text": "Hugging Face has become the go-to hub for sharing machine learning models and datasets. For backend developers, the key advantage is speed: you can load a pretrained summarization model like BART with a few lines and expose it as an HTTP endpoint. In practice, the workflow includes caching model weights on the server, choosing the right device, and setting guardrails such as input length limits and timeouts. Quality depends on parameters like max length, min length, and beam search; higher beams improve fluency but cost latency. In production you often combine summarization with language detection and simple pre- and post-processing, for example stripping markup, normalizing whitespace, and truncating overly long inputs. When domain language differs from general news, you can fine-tune on small curated summaries or use retrieval augmented generation to inject context at request time. Observability also matters: log inputs, outputs, and timing so you can track regressions, and keep a canary set of documents to benchmark changes before rollout.",
  "max_length": 80,
  "min_length": 30,
  "num_beams": 4,
  "no_repeat_ngram_size": 3
}

### summarize (KO) — 한국어 요약 (KoBART)
POST {{baseUrl}}/summarize/ko
Content-Type: application/json

{
  "text": "허깅페이스는 다양한 머신러닝 모델과 데이터셋을 공유하는 오픈 플랫폼을 제공한다. 개발자는 Transformers 라이브러리를 통해 사전학습된 모델을 손쉽게 불러오고, 자신의 데이터로 파인튜닝하여 번역, 요약, 질의응답 등의 애플리케이션을 빠르게 구축할 수 있다.",
  "max_length": 80,
  "min_length": 30
}

### generate (stream)
POST http://127.0.0.1:8000/generate/stream
Content-Type: application/json

{
  "prompt": "Once upon a time",
  "max_new_tokens": 80,
  "temperature": 0.9
}

### OCR — 신분증 이미지 업로드
POST {{baseUrl}}/ocr/id
Content-Type: multipart/form-data; boundary=BOUNDARY

--BOUNDARY
Content-Disposition: form-data; name="file"; filename="id_sample.jpeg"
Content-Type: image/jpeg

< ./id_sample.jpeg

--BOUNDARY--
